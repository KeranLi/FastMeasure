"""
UltraFastSAMç”Ÿäº§çº§ç³»ç»Ÿ
æ–‡ä»¶åï¼šrock_fastsam_system.py
åŠŸèƒ½ï¼šå®Œæ•´çš„ç”Ÿäº§çº§å²©çŸ³é¢—ç²’åˆ†å‰²ç³»ç»Ÿ
"""

import os
import sys
import time
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
import json
import yaml

import torch
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from .yolo_fastsam import UltraSegmentationPipeline
from .seg_tools import ImageProcessor, FileUtils, PerformanceMonitor

# å¯¼å…¥æ¯”ä¾‹å°ºæ£€æµ‹æ¨¡å—
try:
    from scale_detector import ScaleDetector
    SCALE_DETECTOR_AVAILABLE = True
    print("æˆåŠŸå¯¼å…¥æ¯”ä¾‹å°ºæ£€æµ‹æ¨¡å—")
except ImportError as e:
    SCALE_DETECTOR_AVAILABLE = False
    print(f"å¯¼å…¥æ¯”ä¾‹å°ºæ£€æµ‹æ¨¡å—å¤±è´¥: {e}")

# å¯¼å…¥é¢—ç²’æ ‡æ³¨æ¨¡å—
try:
    from grain_marker import add_grain_labels, add_labels_with_config
    GRAIN_MARKER_AVAILABLE = True
    print("æˆåŠŸå¯¼å…¥é¢—ç²’æ ‡æ³¨æ¨¡å—")
except ImportError as e:
    GRAIN_MARKER_AVAILABLE = False
    print(f"å¯¼å…¥é¢—ç²’æ ‡æ³¨æ¨¡å—å¤±è´¥: {e}")

# å¯¼å…¥å¤šç§å‡ ä½•å°ºå¯¸è®¡ç®—å‡½æ•°
from geometry.grain_metric import GrainShapeMetrics

class RockUltraSystem:
    """UltraFastSAMç”Ÿäº§çº§å²©çŸ³åˆ†å‰²ç³»ç»Ÿ"""
    
    VERSION = "1.0.0"
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–UltraFastSAMç³»ç»Ÿ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        print("=" * 70)
        print(f"UltraFastSAMå²©çŸ³é¢—ç²’è‡ªåŠ¨åˆ†å‰²ç³»ç»Ÿ v{self.VERSION}")
        print("=" * 70)
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self._load_config(config_path)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_root = Path(self.config['output']['root_dir'])
        self.output_root.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()
        
        # è®¾ç½®logger
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆå§‹åŒ–æ ¸å¿ƒæµæ°´çº¿
        self.pipeline = UltraSegmentationPipeline(self.config)
        
        # åˆå§‹åŒ–æ¯”ä¾‹å°ºæ£€æµ‹å™¨
        self.scale_detector = None
        if SCALE_DETECTOR_AVAILABLE:
            self._init_scale_detector()
        
        # è¯»å–é¢—ç²’æ ‡æ³¨é…ç½®
        self.grain_label_config = self.config.get('grain_labeling', {})
        if 'bg_color' in self.grain_label_config and self.grain_label_config['bg_color'] == '':
            self.grain_label_config['bg_color'] = None
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        self.processing_history = []
        
        self.logger.info(f"UltraFastSAMç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_root}")
        self.logger.info(f"é…ç½®æ–‡ä»¶: {config_path}")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path(config_path)
        
        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if not config_file.exists():
            print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        
        try:
            config = FileUtils.safe_load_yaml(str(config_file), default={})
            print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
            return config
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'model_paths': {
                'yolo': '../models/best.pt',
                'fastsam': '../models/FastSAM-s.pt',
                'device': 'cpu'
            },
            'scale_detection': {
                'enabled': True,
                'known_length_um': 1000.0,
                'detection_params': {
                    'red_lower1': [0, 120, 120],
                    'red_upper1': [10, 255, 255],
                    'red_lower2': [160, 120, 120],
                    'red_upper2': [180, 255, 255],
                    'crop_height': 220,
                    'crop_width': 600,
                    'search_margin': 80,
                    'min_aspect_ratio': 8,
                    'min_horizontal_score': 0.6
                }
            },
            'processing': {
                'yolo_confidence': 0.25,
                'min_area': 30,
                'min_bbox_area': 20,
                'remove_edge_grains': False,
                'plot_results': True,
                'performance_monitoring': True
            },
            'output': {
                'root_dir': 'results_ultra_fastsam',
                'create_subdirs': True,
                'save_visualization': True,
                'save_mask': True,
                'save_statistics': True,
                'save_summary': True,
                'save_performance': True,
                'save_debug_info': False
            },
            'batch_processing': {
                'supported_formats': ['.tif', '.tiff', '.jpg', '.jpeg', '.png', '.bmp'],
                'skip_corrupted': True,
                'max_workers': 1,
                'log_errors': True
            },
            'logging': {
                'level': 'INFO',
                'save_to_file': True,
                'show_in_console': True,
                'log_format': '%(asctime)s - %(levelname)s - %(message)s'
            },
            'grain_labeling': {
                'enabled': True,
                'font_size': 11,
                'text_color': 'yellow',
                'bg_color': '',
                'show_area': True,
                'max_labels': 1000,
                'min_area': 0,
                'text_outline': True,
                'outline_color': 'black',
                'outline_width': 2.0
            }
        }
    
    def _init_scale_detector(self):
        """åˆå§‹åŒ–æ¯”ä¾‹å°ºæ£€æµ‹å™¨"""
        scale_config = self.config.get('scale_detection', {})
        if scale_config.get('enabled', False) and SCALE_DETECTOR_AVAILABLE:
            try:
                self.scale_detector = ScaleDetector(self.config)
                self.logger.info("æ¯”ä¾‹å°ºæ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"æ¯”ä¾‹å°ºæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.scale_detector = None
        else:
            self.scale_detector = None
            self.logger.info("æ¯”ä¾‹å°ºæ£€æµ‹åŠŸèƒ½å·²ç¦ç”¨")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_config = self.config['logging']
        log_dir = self.output_root / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"ultra_fastsam_{timestamp}.log"
        
        log_level = getattr(logging, log_config['level'])
        
        # é…ç½®æ ¹logger
        logger = logging.getLogger()
        logger.setLevel(log_level)
        logger.handlers.clear()
        
        # æ–‡ä»¶handler
        if log_config['save_to_file']:
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_formatter = logging.Formatter(log_config.get('log_format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
        
        # æ§åˆ¶å°handler
        if log_config['show_in_console']:
            console_handler = logging.StreamHandler(sys.stdout)
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    
    def initialize_models(self) -> bool:
        """åˆå§‹åŒ–AIæ¨¡å‹"""
        self.performance_monitor.start_timing('initialize_models')
        
        model_paths = self.config['model_paths']
        device = model_paths.get('device', 'cpu')
        
        self.logger.info(f"åˆå§‹åŒ–AIæ¨¡å‹ (è®¾å¤‡: {device})")
        
        try:
            success = self.pipeline.load_models(
                yolo_path=model_paths['yolo'],
                fastsam_path=model_paths['fastsam'],
                device=device
            )
            
            if success:
                self.performance_monitor.end_timing('initialize_models')
                self.logger.info("AIæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                self.logger.error("AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
                return False
                
        except Exception as e:
            self.performance_monitor.end_timing('initialize_models')
            self.logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            return False
    
    def process_single_image(self, image_path: str) -> Dict[str, Any]:
        """
        å¤„ç†å•å¼ å²©çŸ³å›¾ç‰‡
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        result = {
            'image_path': str(image_path),
            'image_name': Path(image_path).name,
            'success': False,
            'grains_count': 0,
            'error_message': None,
            'output_files': [],
            'processing_time': 0,
            'performance_metrics': {},
            'timestamp': datetime.now().isoformat(),
            'scale_factor': None,
            'scale_detection_success': False,
            'system_version': self.VERSION
        }
        
        self.performance_monitor.start_timing('total_processing')
        
        try:
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
            self.logger.info(f"å¤„ç†å›¾ç‰‡: {image_path}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = self.create_output_structure(Path(image_path))
            
            # åŠ è½½å›¾ç‰‡
            self.performance_monitor.start_timing('image_loading')
            image = ImageProcessor.load_image_safely(image_path)
            
            if image is None:
                result['error_message'] = "æ— æ³•åŠ è½½å›¾ç‰‡"
                result['processing_time'] = self.performance_monitor.timings.get('total_processing', {}).get('elapsed', 0)
                self.logger.error(f"å›¾ç‰‡åŠ è½½å¤±è´¥: {image_path}")
                return result
            
            # éªŒè¯å›¾åƒæ•°æ®
            is_valid, valid_msg = ImageProcessor.validate_image(image)
            if not is_valid:
                result['error_message'] = valid_msg
                result['processing_time'] = self.performance_monitor.timings.get('total_processing', {}).get('elapsed', 0)
                self.logger.error(f"å›¾åƒæ•°æ®éªŒè¯å¤±è´¥: {valid_msg}")
                return result
            
            self.performance_monitor.end_timing('image_loading')
            self.logger.info(f"å›¾ç‰‡åŠ è½½æˆåŠŸ: {image.shape}")
            
            # æ£€æµ‹æ¯”ä¾‹å°º
            scale_factor = None
            scale_detection_success = False
            
            if self.scale_detector and SCALE_DETECTOR_AVAILABLE:
                try:
                    self.performance_monitor.start_timing('scale_detection')
                    self.logger.info("æ£€æµ‹å›¾ç‰‡ä¸­çš„æ¯”ä¾‹å°º...")
                    
                    scale_factor, scale_success = self.scale_detector.detect(image_path)
                    
                    if scale_success:
                        result['scale_factor'] = float(scale_factor)
                        result['scale_detection_success'] = True
                        scale_detection_success = True
                        self.logger.info(f"æ¯”ä¾‹å°ºæ£€æµ‹æˆåŠŸ: {scale_factor:.4f} Î¼m/px")
                    else:
                        self.logger.warning("æ¯”ä¾‹å°ºæ£€æµ‹å¤±è´¥ï¼Œä»…è¾“å‡ºåƒç´ é¢ç§¯")
                    
                    self.performance_monitor.end_timing('scale_detection')
                except Exception as e:
                    self.logger.warning(f"æ¯”ä¾‹å°ºæ£€æµ‹å¼‚å¸¸: {e}")
            
            # è·å–å¤„ç†å‚æ•°
            processing_config = self.config['processing']
            
            # è¿è¡ŒUltraFastSAMåˆ†å‰²
            self.performance_monitor.start_timing('ultra_segmentation')
            
            all_grains, labels, mask_all, grain_data, fig, ax = self.pipeline.ultra_segmentation(
                image=image,
                conf_threshold=processing_config['yolo_confidence'],
                min_area=processing_config['min_area'],
                min_bbox_area=processing_config['min_bbox_area'],
                remove_edge_grains=processing_config['remove_edge_grains'],
                plot_image=processing_config['plot_results']
            )
            
            self.performance_monitor.end_timing('ultra_segmentation')
            
            # æ›´æ–°ç»“æœ
            result['grains_count'] = len(all_grains)
            result['success'] = True

            # è®¡ç®—é¢—ç²’çš„å½¢çŠ¶å‚æ•°
            #print(grain_data.columns)  # æ‰“å°åˆ—åä»¥ç¡®è®¤æ˜¯å¦åŒ…å« 'coordinates' åˆ—
            shape_calculator = GrainShapeMetrics(grain_data)  # åˆ›å»ºGrainShapeMetricså®ä¾‹
            grain_data = shape_calculator.compute_all_metrics()  # è®¡ç®—æ‰€æœ‰å½¢çŠ¶å‚æ•°
            
            # ä¿å­˜ç»“æœæ–‡ä»¶
            output_files = []
            
            # ä¿å­˜å¯è§†åŒ–ç»“æœ
            if self.config['output']['save_visualization'] and fig is not None:
                # 1. ä¿å­˜åŸå§‹åˆ†å‰²ç»“æœ
                plot_path = output_dir / "segmentation_result.png"
                fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
                output_files.append(str(plot_path))
                self.logger.info(f"åŸå§‹ç»“æœå›¾ä¿å­˜è‡³: {plot_path}")
                
                # 2. ä¿å­˜å¸¦æ ‡æ³¨çš„ç»“æœå›¾
                if (GRAIN_MARKER_AVAILABLE and 
                    self.grain_label_config.get('enabled', True) and 
                    grain_data is not None and 
                    not grain_data.empty):
                    
                    try:
                        # åˆ›å»ºå¸¦æ ‡æ³¨çš„å›¾åƒ
                        fig_labeled, ax_labeled = plt.subplots(figsize=(15, 10))
                        ax_labeled.imshow(image)
                        ax_labeled.axis('off')
                        
                        # æ·»åŠ é¢—ç²’æ ‡æ³¨
                        if 'add_labels_with_config' in globals():
                            ax_labeled = add_labels_with_config(
                                ax=ax_labeled,
                                grain_data=grain_data,
                                image_shape=image.shape,
                                config=self.grain_label_config
                            )
                        
                        # éšè—åæ ‡è½´å’Œè¾¹æ¡†
                        ax_labeled.set_xticks([])
                        ax_labeled.set_yticks([])
                        ax_labeled.set_xlim([0, image.shape[1]])
                        ax_labeled.set_ylim([image.shape[0], 0])
                        plt.tight_layout()
                        
                        # ä¿å­˜å¸¦æ ‡æ³¨çš„å›¾
                        labeled_path = output_dir / "segmentation_labeled.png"
                        fig_labeled.savefig(labeled_path, dpi=300, bbox_inches='tight', 
                                           pad_inches=0, facecolor='white')
                        output_files.append(str(labeled_path))
                        plt.close(fig_labeled)
                        
                        self.logger.info(f"å¸¦æ ‡æ³¨ç»“æœå›¾ä¿å­˜è‡³: {labeled_path}")
                        
                    except Exception as e:
                        self.logger.warning(f"ç”Ÿæˆå¸¦æ ‡æ³¨ç»“æœå›¾å¤±è´¥: {e}")
                
                # å…³é—­åŸå§‹å›¾å½¢
                plt.close(fig)
            
            # 3. ä¿å­˜åˆ†å‰²æ©ç 
            if self.config['output']['save_mask'] and mask_all is not None and np.max(mask_all) > 0:
                mask_path = output_dir / "segmentation_mask.png"
                mask_uint8 = (mask_all > 0).astype(np.uint8) * 255
                Image.fromarray(mask_uint8).save(mask_path)
                output_files.append(str(mask_path))
                self.logger.info(f"åˆ†å‰²æ©ç ä¿å­˜è‡³: {mask_path}")
            
            # ä¿å­˜ç»Ÿè®¡è¡¨æ ¼
            if self.config['output']['save_statistics'] and grain_data is not None and not grain_data.empty:
                # ç¡®ä¿æ˜¯DataFrame
                if not isinstance(grain_data, pd.DataFrame):
                    grain_data = pd.DataFrame(grain_data)
                
                # å¦‚æœæ¯”ä¾‹å°ºæ£€æµ‹æˆåŠŸï¼Œè®¡ç®—çœŸå®é¢ç§¯
                if scale_detection_success and scale_factor:
                    if 'area' in grain_data.columns:
                        grain_data['area'] = pd.to_numeric(grain_data['area'], errors='coerce')
                        valid_areas = grain_data['area'].dropna()
                        
                        if len(valid_areas) > 0:
                            grain_data['area_um2'] = valid_areas * (scale_factor ** 2)
                            grain_data['diameter_um'] = 2 * np.sqrt(grain_data['area_um2'] / np.pi)
                
                # ä¿å­˜CSV
                csv_path = output_dir / "grain_statistics.csv"
                grain_data.to_csv(csv_path, index=False, encoding='utf-8')
                output_files.append(str(csv_path))
                self.logger.info(f"é¢—ç²’æ•°æ®ä¿å­˜è‡³: {csv_path}")
                
                # ä¿å­˜JSONæ±‡æ€»ä¿¡æ¯
                if self.config['output']['save_summary']:
                    summary = self._create_summary_dict(
                        image_path, image, grain_data, scale_detection_success, scale_factor
                    )
                    
                    json_path = output_dir / "summary.json"
                    FileUtils.safe_save_json(summary, str(json_path))
                    output_files.append(str(json_path))
            
            # ä¿å­˜æ€§èƒ½æ•°æ®
            if self.config['output']['save_performance']:
                performance_data = self.pipeline.get_performance()
                perf_path = output_dir / "performance.json"
                FileUtils.safe_save_json(performance_data, str(perf_path))
                output_files.append(str(perf_path))
            
            # ä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.config['output'].get('save_debug_info', False):
                debug_info = {
                    'image_shape': image.shape,
                    'num_yolo_boxes': len(all_grains),
                    'scale_factor': scale_factor,
                    'config': self.config
                }
                
                debug_path = output_dir / "debug_info.json"
                FileUtils.safe_save_json(debug_info, str(debug_path))
                output_files.append(str(debug_path))
            
            result['output_files'] = output_files
            
        except Exception as e:
            result['success'] = False
            result['error_message'] = str(e)
            self.logger.error(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {image_path}")
            self.logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
            self.logger.error(traceback.format_exc())
        
        finally:
            # ç»“æŸæ€»è®¡æ—¶
            self.performance_monitor.end_timing('total_processing')
            
            # è®¡ç®—æ€»å¤„ç†æ—¶é—´
            total_time = self.performance_monitor.timings.get('total_processing', {}).get('elapsed', 0)
            result['processing_time'] = total_time
            
            # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
            result['performance_metrics'] = self.performance_monitor.get_summary()
            
            # è®°å½•å¤„ç†å†å²
            self.processing_history.append(result.copy())
            
            self.logger.info(f"å›¾ç‰‡å¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        
        return result
    
    def create_output_structure(self, image_path: Path) -> Path:
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        if self.config['output']['create_subdirs']:
            image_name = image_path.stem
            output_dir = self.output_root / "images" / image_name
        else:
            output_dir = self.output_root
        
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir
    
    def _create_summary_dict(self, image_path, image, grain_data, scale_success, scale_factor):
        """åˆ›å»ºæ±‡æ€»ä¿¡æ¯å­—å…¸"""
        summary = {
            'image_name': Path(image_path).name,
            'image_size': {
                'height': image.shape[0],
                'width': image.shape[1],
                'channels': image.shape[2]
            },
            'total_grains': int(len(grain_data)),
            'processing_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'system_version': self.VERSION
        }
        
        # é¢ç§¯ç»Ÿè®¡
        if 'area' in grain_data.columns:
            grain_data['area'] = pd.to_numeric(grain_data['area'], errors='coerce')
            valid_areas = grain_data['area'].dropna()
            
            if len(valid_areas) > 0:
                summary['area_statistics_pixels'] = {
                    'total': float(valid_areas.sum()),
                    'average': float(valid_areas.mean()),
                    'min': float(valid_areas.min()),
                    'max': float(valid_areas.max()),
                    'std': float(valid_areas.std())
                }
        
        # çœŸå®é¢ç§¯ç»Ÿè®¡
        if scale_success:
            summary['scale_detection'] = {
                'success': True,
                'scale_factor_um_per_px': float(scale_factor)
            }
            
            if 'area_um2' in grain_data.columns:
                grain_data['area_um2'] = pd.to_numeric(grain_data['area_um2'], errors='coerce')
                valid_areas_um2 = grain_data['area_um2'].dropna()
                
                if len(valid_areas_um2) > 0:
                    summary['area_statistics_um2'] = {
                        'total': float(valid_areas_um2.sum()),
                        'average': float(valid_areas_um2.mean()),
                        'min': float(valid_areas_um2.min()),
                        'max': float(valid_areas_um2.max())
                    }
        
        return summary
    
    def batch_process(self, input_folder: str) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å›¾ç‰‡
        
        Args:
            input_folder: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç†: {input_folder}")
        
        # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
        input_path = Path(input_folder)
        if not input_path.exists():
            self.logger.error(f"è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
            return {'success': False, 'error': 'è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨'}
        
        # è·å–æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        supported_formats = self.config['batch_processing']['supported_formats']
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = []
        for format_ext in supported_formats:
            image_files.extend(input_path.rglob(f"*{format_ext}"))
            image_files.extend(input_path.rglob(f"*{format_ext.upper()}"))
        
        image_files = list(set(image_files))
        
        if not image_files:
            self.logger.error(f"æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶: {input_folder}")
            return {'success': False, 'error': 'æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶'}
        
        self.logger.info(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
        
        # æ‰¹é‡å¤„ç†ç»“æœ
        batch_results = {
            'total': len(image_files),
            'success': 0,
            'failed': 0,
            'failed_images': [],
            'total_grains': 0,
            'processing_start': datetime.now().isoformat(),
            'individual_results': []
        }
        
        # å¤„ç†æ¯å¼ å›¾ç‰‡
        for i, image_file in enumerate(image_files, 1):
            self.logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(image_files)} - {image_file.name}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸå
            skip_corrupted = self.config['batch_processing']['skip_corrupted']
            if skip_corrupted:
                image_data = ImageProcessor.load_image_safely(str(image_file))
                if image_data is None:
                    self.logger.warning(f"è·³è¿‡æŸåå›¾ç‰‡: {image_file.name}")
                    batch_results['failed'] += 1
                    batch_results['failed_images'].append({
                        'path': str(image_file),
                        'error': 'æ–‡ä»¶æŸå',
                        'skipped': True
                    })
                    continue
            
            # å¤„ç†å•å¼ å›¾ç‰‡
            result = self.process_single_image(str(image_file))
            batch_results['individual_results'].append(result)
            
            if result['success']:
                batch_results['success'] += 1
                batch_results['total_grains'] += result['grains_count']
                self.logger.info(f"æˆåŠŸ: {image_file.name} ({result['grains_count']}ä¸ªé¢—ç²’)")
            else:
                batch_results['failed'] += 1
                batch_results['failed_images'].append({
                    'path': str(image_file),
                    'error': result['error_message'],
                    'skipped': False
                })
                self.logger.warning(f"å¤±è´¥: {image_file.name} - {result['error_message']}")
        
        batch_results['processing_end'] = datetime.now().isoformat()
        
        # ç”Ÿæˆæ‰¹é‡æŠ¥å‘Š
        self._generate_batch_report(batch_results)
        
        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {batch_results['success']}/{batch_results['total']} æˆåŠŸ")
        
        return batch_results
    
    def _generate_batch_report(self, batch_results: Dict[str, Any]):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        report_path = self.output_root / "batch_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("UltraFastSAMæ‰¹é‡å¤„ç†æŠ¥å‘Š\n")
            f.write("=" * 70 + "\n\n")
            
            f.write(f"å¤„ç†å¼€å§‹æ—¶é—´: {batch_results['processing_start']}\n")
            f.write(f"å¤„ç†ç»“æŸæ—¶é—´: {batch_results['processing_end']}\n")
            f.write(f"å¤„ç†æ€»æ—¶é•¿: {self._calculate_duration(batch_results['processing_start'], batch_results['processing_end'])}\n\n")
            
            f.write(f"æ€»å›¾ç‰‡æ•°: {batch_results['total']}\n")
            f.write(f"æˆåŠŸå¤„ç†: {batch_results['success']}\n")
            f.write(f"å¤„ç†å¤±è´¥: {batch_results['failed']}\n")
            f.write(f"æ€»æ£€æµ‹é¢—ç²’æ•°: {batch_results['total_grains']}\n\n")
            
            if batch_results['failed'] > 0:
                f.write("å¤±è´¥/è·³è¿‡å›¾ç‰‡åˆ—è¡¨:\n")
                f.write("-" * 70 + "\n")
                for i, fail in enumerate(batch_results['failed_images'], 1):
                    f.write(f"{i}. å›¾ç‰‡: {fail['path']}\n")
                    if fail.get('skipped', False):
                        f.write(f"   åŸå› : æ–‡ä»¶æŸåï¼Œå·²è·³è¿‡ - {fail['error']}\n")
                    else:
                        f.write(f"   åŸå› : å¤„ç†å¤±è´¥ - {fail['error']}\n")
                    f.write("-" * 70 + "\n")
            
            successful_results = [r for r in batch_results['individual_results'] if r.get('success')]
            if successful_results:
                f.write("\næˆåŠŸå¤„ç†å›¾ç‰‡ç»Ÿè®¡:\n")
                f.write("-" * 70 + "\n")
                for i, result in enumerate(successful_results, 1):
                    f.write(f"{i}. {result['image_name']}\n")
                    f.write(f"   é¢—ç²’æ•°: {result['grains_count']}\n")
                    f.write(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’\n")
                    if result.get('scale_detection_success', False):
                        f.write(f"   æ¯”ä¾‹å› å­: {result.get('scale_factor', 'N/A')} Î¼m/px\n")
                    if result['output_files']:
                        f.write(f"   è¾“å‡ºæ–‡ä»¶: {len(result['output_files'])}ä¸ª\n")
                f.write("-" * 70 + "\n")
        
        # ä¿å­˜JSONæ ¼å¼çš„æŠ¥å‘Š
        json_report_path = self.output_root / "batch_report.json"
        FileUtils.safe_save_json(batch_results, str(json_report_path))
        
        self.logger.info(f"æ‰¹é‡å¤„ç†æŠ¥å‘Šä¿å­˜è‡³: {report_path}")
    
    def _calculate_duration(self, start_iso: str, end_iso: str) -> str:
        """è®¡ç®—å¤„ç†æ—¶é•¿"""
        try:
            start_time = datetime.fromisoformat(start_iso)
            end_time = datetime.fromisoformat(end_iso)
            duration = end_time - start_time
            
            hours = duration.seconds // 3600
            minutes = (duration.seconds % 3600) // 60
            seconds = duration.seconds % 60
            
            return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’"
        except:
            return "æœªçŸ¥"
    
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print("=" * 70)
        print(f"ğŸ­ UltraFastSAMå²©çŸ³é¢—ç²’è‡ªåŠ¨åˆ†å‰²ç³»ç»Ÿ v{self.VERSION}")
        print("=" * 70)
        print(f"ç³»ç»Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è¾“å‡ºç›®å½•: {self.output_root}")
        print(f"è®¾å¤‡æ¨¡å¼: {self.config['model_paths'].get('device', 'cpu')}")
        print(f"YOLOæ¨¡å‹: {self.config['model_paths']['yolo']}")
        print(f"FastSAMæ¨¡å‹: {self.config['model_paths']['fastsam']}")
        
        scale_config = self.config.get('scale_detection', {})
        if scale_config.get('enabled', False):
            print(f"æ¯”ä¾‹å°ºæ£€æµ‹: å·²å¯ç”¨")
            print(f"å·²çŸ¥é•¿åº¦: {scale_config.get('known_length_um', 'N/A')} Î¼m")
        else:
            print(f"æ¯”ä¾‹å°ºæ£€æµ‹: å·²ç¦ç”¨")
        
        if self.grain_label_config.get('enabled', True):
            print(f"é¢—ç²’æ ‡æ³¨: å·²å¯ç”¨")
            bg_color = self.grain_label_config.get('bg_color', '')
            if bg_color is None or bg_color == '':
                print(f"æ ‡æ³¨æ ·å¼: æ— èƒŒæ™¯, {self.grain_label_config.get('font_size', 11)}pxé»„è‰²æ–‡å­—")
            else:
                print(f"æ ‡æ³¨æ ·å¼: æœ‰èƒŒæ™¯, {self.grain_label_config.get('font_size', 9)}pxé»‘è‰²æ–‡å­—")
            print(f"æœ€å¤§æ ‡æ³¨æ•°: {self.grain_label_config.get('max_labels', 1000)}")
        else:
            print(f"é¢—ç²’æ ‡æ³¨: å·²ç¦ç”¨")
        
        print("=" * 70)
    
    def get_processing_history(self) -> List[Dict[str, Any]]:
        """è·å–å¤„ç†å†å²"""
        return self.processing_history.copy()
    
    def clear_processing_history(self):
        """æ¸…ç©ºå¤„ç†å†å²"""
        self.processing_history = []
        self.logger.info("å¤„ç†å†å²å·²æ¸…ç©º")


if __name__ == "__main__":
    print("è¿™æ˜¯ä¸€ä¸ªæ¨¡å—æ–‡ä»¶ï¼Œè¯·é€šè¿‡ run_fastsam.py æ¥å¯åŠ¨ç³»ç»Ÿ")
    print("æˆ–è€…ç›´æ¥ä½¿ç”¨:")
    print("  from rock_fastsam_system import RockUltraSystem")
    print("  system = RockUltraSystem()")
    print("  system.initialize_models()")
    print("  system.process_single_image('å›¾ç‰‡è·¯å¾„')")