"""
SuperFastSAM
æ–‡ä»¶åï¼šrock_super_fastsam.py
"""

import os
import sys
import time
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import json
import yaml

import torch
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from ultralytics import YOLO

# å¯¼å…¥SuperFastSAMå¼•æ“
try:
    from fastsam_optimized import SuperFastSAM
    from yolo_super_fastsam import yolo_super_fastsam_segmentation
    print("âœ… æˆåŠŸå¯¼å…¥SuperFastSAMå¼•æ“")
except ImportError as e:
    print(f"âŒ å¯¼å…¥SuperFastSAMå¼•æ“å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥å·¥å…·å‡½æ•°
try:
    from utils import (
        check_image_file_pro,
        validate_image_data,
        convert_to_rgb,
        normalize_image
    )
    print("âœ… æˆåŠŸå¯¼å…¥å·¥å…·å‡½æ•°")
except ImportError:
    print("âš ï¸ ä½¿ç”¨ç®€åŒ–å·¥å…·å‡½æ•°")
    from utils_simple import *

# å¯¼å…¥æ¯”ä¾‹å°ºæ£€æµ‹æ¨¡å—
try:
    from scale_detector import ScaleDetector
    SCALE_DETECTOR_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥æ¯”ä¾‹å°ºæ£€æµ‹æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥æ¯”ä¾‹å°ºæ£€æµ‹æ¨¡å—å¤±è´¥: {e}")
    SCALE_DETECTOR_AVAILABLE = False

# å¯¼å…¥é¢—ç²’æ ‡æ³¨æ¨¡å—
try:
    from grain_marker import add_grain_labels, add_labels_with_config
    GRAIN_MARKER_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥é¢—ç²’æ ‡æ³¨æ¨¡å—")
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥é¢—ç²’æ ‡æ³¨æ¨¡å—å¤±è´¥: {e}")
    GRAIN_MARKER_AVAILABLE = False


class RockSegmentationSystemSuper:
    """SuperFastSAMç”Ÿäº§çº§å²©çŸ³åˆ†å‰²ç³»ç»Ÿ"""
    
    VERSION = "1.0.0"
    
    def __init__(self, config_path: str = "new/config_super_fastsam.yaml"):
        """
        åˆå§‹åŒ–SuperFastSAMç³»ç»Ÿ
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # å…ˆåˆå§‹åŒ–logger
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        print("=" * 60)
        print(f"ğŸ­ SuperFastSAMå²©çŸ³åˆ†å‰²ç³»ç»Ÿ v{self.VERSION}")
        print("=" * 60)
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self._load_config(config_path)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        self.output_root = Path(self.config['output']['root_dir'])
        self.output_root.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.yolo_model = None
        self.super_fastsam = None
        
        # åˆå§‹åŒ–æ¯”ä¾‹å°ºæ£€æµ‹å™¨
        self.scale_detector = None
        if SCALE_DETECTOR_AVAILABLE:
            self._init_scale_detector()
        
        # è¯»å–é¢—ç²’æ ‡æ³¨é…ç½®
        self.grain_label_config = self.config.get('grain_labeling', {})
        
        # å¤„ç†ç©ºå­—ç¬¦ä¸²èƒŒæ™¯ä¸ºNone
        if 'bg_color' in self.grain_label_config and self.grain_label_config['bg_color'] == '':
            self.grain_label_config['bg_color'] = None
        
        self.logger.info(f"SuperFastSAMç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_root}")
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path(config_path)
        if not config_file.is_absolute():
            config_file = Path(__file__).parent / Path(config_path).name
        
        if not config_file.exists():
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
            return config
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'model_paths': {
                'yolo': '../models/best.pt',
                'fastsam': '../models/FastSAM-s.pt',
                'device': 'cpu'
            },
            'scale_detection': {
                'enabled': True,
                'known_length_um': 1000.0,
                'detection_params': {
                    'red_lower1': [0, 120, 120],
                    'red_upper1': [10, 255, 255],
                    'red_lower2': [160, 120, 120],
                    'red_upper2': [180, 255, 255],
                    'crop_height': 220,
                    'crop_width': 600,
                    'search_margin': 80,
                    'min_aspect_ratio': 8,
                    'min_horizontal_score': 0.6
                }
            },
            'processing': {
                'yolo_confidence': 0.25,
                'fastsam_confidence': 0.35,
                'min_area': 30,
                'min_bbox_area': 20,
                'remove_edge_grains': False,
                'plot_results': True,
                'performance_monitor': True
            },
            'output': {
                'root_dir': 'results_super_fastsam',
                'create_subdirs': True,
                'save_visualization': True,
                'save_mask': True,
                'save_statistics': True,
                'save_summary': True,
                'save_performance': True
            },
            'batch_processing': {
                'supported_formats': ['.tif', '.tiff', '.jpg', '.jpeg', '.png', '.bmp'],
                'skip_corrupted': True,
                'max_workers': 1,
                'log_errors': True
            },
            'logging': {
                'level': 'INFO',
                'save_to_file': True,
                'show_in_console': True,
                'log_format': '%(asctime)s - %(levelname)s - %(message)s'
            },
            'grain_labeling': {
                'enabled': True,
                'font_size': 11,
                'text_color': 'yellow',
                'bg_color': '',
                'show_area': True,
                'max_labels': 1000,
                'min_area': 0,
                'text_outline': True,
                'outline_color': 'black',
                'outline_width': 2.0
            },
            'performance': {
                'enable_monitoring': True,
                'save_timings': True,
                'alert_threshold_sec': 300
            }
        }
    
    def _init_scale_detector(self):
        """åˆå§‹åŒ–æ¯”ä¾‹å°ºæ£€æµ‹å™¨"""
        scale_config = self.config.get('scale_detection', {})
        if scale_config.get('enabled', False) and SCALE_DETECTOR_AVAILABLE:
            try:
                self.scale_detector = ScaleDetector(self.config)
                self.logger.info("âœ… æ¯”ä¾‹å°ºæ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"æ¯”ä¾‹å°ºæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.scale_detector = None
        else:
            self.scale_detector = None
            self.logger.info("æ¯”ä¾‹å°ºæ£€æµ‹åŠŸèƒ½å·²ç¦ç”¨")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_config = self.config['logging']
        log_dir = self.output_root / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"super_fastsam_{timestamp}.log"
        
        log_level = getattr(logging, log_config['level'])
        
        # é…ç½®æ ¹logger
        logger = logging.getLogger()
        logger.setLevel(log_level)
        logger.handlers.clear()
        
        # æ–‡ä»¶handler
        if log_config['save_to_file']:
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_formatter = logging.Formatter(log_config.get('log_format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)
        
        # æ§åˆ¶å°handler
        if log_config['show_in_console']:
            console_handler = logging.StreamHandler(sys.stdout)
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
        
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    
    def initialize_models(self) -> bool:
        """åˆå§‹åŒ–YOLOå’ŒSuperFastSAMæ¨¡å‹"""
        self.logger.info("=" * 50)
        self.logger.info("åˆå§‹åŒ–SuperFastSAM AIæ¨¡å‹...")
        
        model_paths = self.config['model_paths']
        device = model_paths.get('device', 'cpu')
        
        self.logger.info(f"è¿è¡Œè®¾å¤‡: {device}")
        
        try:
            # åŠ è½½YOLOæ¨¡å‹
            yolo_path = model_paths['yolo']
            if not Path(yolo_path).exists():
                self.logger.error(f"YOLOæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {yolo_path}")
                return False
            
            self.logger.info(f"åŠ è½½YOLOæ¨¡å‹: {yolo_path}")
            self.yolo_model = YOLO(yolo_path)
            self.logger.info("âœ… YOLOæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½SuperFastSAMå¼•æ“
            fastsam_path = model_paths['fastsam']
            if not Path(fastsam_path).exists():
                self.logger.error(f"FastSAMæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {fastsam_path}")
                return False
            
            self.logger.info(f"åŠ è½½SuperFastSAMå¼•æ“: {fastsam_path}")
            self.super_fastsam = SuperFastSAM(
                model_path=fastsam_path,
                device=device
            )
            self.logger.info("âœ… SuperFastSAMå¼•æ“åŠ è½½æˆåŠŸ")
            
            self.logger.info("=" * 50)
            return True
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            return False
    
    def process_single_image(self, image_path: str) -> Dict:
        """
        å¤„ç†å•å¼ å²©çŸ³å›¾ç‰‡
        """
        result = {
            'image_path': str(image_path),
            'image_name': Path(image_path).name,
            'success': False,
            'grains_count': 0,
            'error_message': None,
            'output_files': [],
            'processing_time': 0,
            'performance_metrics': {},
            'timestamp': datetime.now().isoformat(),
            'scale_factor': None,
            'scale_detection_success': False
        }
        
        start_time = time.time()
        
        try:
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶
            is_valid, message = check_image_file_pro(image_path)
            if not is_valid:
                self.logger.warning(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶æ£€æŸ¥è­¦å‘Š: {image_path} - {message}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = self.create_output_structure(Path(image_path))
            
            # åŠ è½½å›¾ç‰‡
            self.logger.info(f"ğŸ–¼ï¸  åŠ è½½å›¾ç‰‡: {image_path}")
            image = self._load_image_safely(image_path)
            
            if image is None:
                result['error_message'] = "æ— æ³•åŠ è½½å›¾ç‰‡"
                result['processing_time'] = time.time() - start_time
                self.logger.error(f"âŒ å›¾ç‰‡åŠ è½½å¤±è´¥: {image_path}")
                return result
            
            # éªŒè¯å›¾åƒæ•°æ®
            is_valid, valid_msg = validate_image_data(image)
            if not is_valid:
                result['error_message'] = valid_msg
                result['processing_time'] = time.time() - start_time
                self.logger.error(f"âŒ å›¾åƒæ•°æ®éªŒè¯å¤±è´¥: {valid_msg}")
                return result
            
            self.logger.info(f"ğŸ“Š å›¾ç‰‡æœ€ç»ˆå°ºå¯¸: {image.shape}, æ•°æ®ç±»å‹: {image.dtype}")
            
            # æ£€æµ‹æ¯”ä¾‹å°º
            scale_factor = None
            scale_detection_success = False
            
            if self.scale_detector and SCALE_DETECTOR_AVAILABLE:
                try:
                    self.logger.info("æ£€æµ‹å›¾ç‰‡ä¸­çš„æ¯”ä¾‹å°º...")
                    scale_factor, scale_success = self.scale_detector.detect(image_path)
                    
                    if scale_success:
                        result['scale_factor'] = float(scale_factor)
                        result['scale_detection_success'] = True
                        scale_detection_success = True
                        self.logger.info(f"æ¯”ä¾‹å°ºæ£€æµ‹æˆåŠŸ: {scale_factor:.4f} Î¼m/px")
                    else:
                        self.logger.warning("æ¯”ä¾‹å°ºæ£€æµ‹å¤±è´¥ï¼Œä»…è¾“å‡ºåƒç´ é¢ç§¯")
                except Exception as e:
                    self.logger.warning(f"æ¯”ä¾‹å°ºæ£€æµ‹å¼‚å¸¸: {e}")
            
            # è·å–å¤„ç†å‚æ•°
            processing_config = self.config['processing']
            
            # è¿è¡ŒSuperFastSAMåˆ†å‰²æµæ°´çº¿
            all_grains, labels, mask_all, grain_data, fig, ax = yolo_super_fastsam_segmentation(
                image=image,
                yolo_model=self.yolo_model,
                super_fastsam=self.super_fastsam,
                conf_threshold=processing_config['yolo_confidence'],
                min_area=processing_config['min_area'],
                min_bbox_area=processing_config['min_bbox_area'],
                remove_edge_grains=processing_config['remove_edge_grains'],
                plot_image=processing_config['plot_results'],
                class_id=None
            )
            
            # æ›´æ–°ç»“æœ
            result['grains_count'] = len(all_grains)
            result['success'] = True
            
            # ä¿å­˜ç»“æœæ–‡ä»¶
            output_files = []
            
            # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆä¸é¡¹ç›®ä¸€å®Œå…¨ç›¸åŒçš„ä¸‰å¼ å›¾ï¼‰
            if self.config['output']['save_visualization'] and fig is not None:
                # 1. ç¬¬ä¸€å¼ å›¾ï¼šåŸå§‹åˆ†å‰²ç»“æœ
                plot_path = output_dir / "segmentation_result.png"
                fig.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
                output_files.append(str(plot_path))
                self.logger.info(f"åŸå§‹ç»“æœå›¾ä¿å­˜è‡³: {plot_path}")
                
                # 2. ç¬¬äºŒå¼ å›¾ï¼šå¸¦æ ‡æ³¨çš„ç»“æœå›¾
                if (GRAIN_MARKER_AVAILABLE and 
                    self.grain_label_config.get('enabled', True) and 
                    grain_data is not None and 
                    not grain_data.empty):
                    
                    try:
                        # åˆ›å»ºå¸¦æ ‡æ³¨çš„å›¾åƒ
                        fig_labeled, ax_labeled = plt.subplots(figsize=(15, 10))
                        ax_labeled.imshow(image)
                        ax_labeled.axis('off')
                        
                        # æ·»åŠ é¢—ç²’æ ‡æ³¨ï¼ˆä¸é¡¹ç›®ä¸€ç›¸åŒï¼‰
                        if 'add_labels_with_config' in globals():
                            ax_labeled = add_labels_with_config(
                                ax=ax_labeled,
                                grain_data=grain_data,
                                image_shape=image.shape,
                                config=self.grain_label_config
                            )
                        
                        # éšè—åæ ‡è½´å’Œè¾¹æ¡†
                        ax_labeled.set_xticks([])
                        ax_labeled.set_yticks([])
                        ax_labeled.set_xlim([0, image.shape[1]])
                        ax_labeled.set_ylim([image.shape[0], 0])
                        plt.tight_layout()
                        
                        # ä¿å­˜å¸¦æ ‡æ³¨çš„å›¾
                        labeled_path = output_dir / "segmentation_labeled.png"
                        fig_labeled.savefig(labeled_path, dpi=300, bbox_inches='tight', 
                                           pad_inches=0, facecolor='white')
                        output_files.append(str(labeled_path))
                        plt.close(fig_labeled)
                        
                        self.logger.info(f"å¸¦æ ‡æ³¨ç»“æœå›¾ä¿å­˜è‡³: {labeled_path}")
                        
                    except Exception as e:
                        self.logger.warning(f"ç”Ÿæˆå¸¦æ ‡æ³¨ç»“æœå›¾å¤±è´¥: {e}")
                
                # 3. å…³é—­åŸå§‹å›¾å½¢
                plt.close(fig)
            
            # 3. ç¬¬ä¸‰å¼ å›¾ï¼šåˆ†å‰²æ©ç å›¾
            if self.config['output']['save_mask'] and mask_all is not None and np.max(mask_all) > 0:
                mask_path = output_dir / "segmentation_mask.png"
                mask_uint8 = (mask_all > 0).astype(np.uint8) * 255
                Image.fromarray(mask_uint8).save(mask_path)
                output_files.append(str(mask_path))
                self.logger.info(f"åˆ†å‰²æ©ç ä¿å­˜è‡³: {mask_path}")
            
            # ä¿å­˜ç»Ÿè®¡è¡¨æ ¼ï¼ˆä¸é¡¹ç›®ä¸€æ ¼å¼ç›¸åŒï¼‰
            if self.config['output']['save_statistics'] and grain_data is not None and not grain_data.empty:
                # ç¡®ä¿æ˜¯DataFrame
                if not isinstance(grain_data, pd.DataFrame):
                    grain_data = pd.DataFrame(grain_data)
                
                # å¦‚æœæ¯”ä¾‹å°ºæ£€æµ‹æˆåŠŸï¼Œè®¡ç®—çœŸå®é¢ç§¯
                if scale_detection_success and scale_factor:
                    if 'area' in grain_data.columns:
                        grain_data['area'] = pd.to_numeric(grain_data['area'], errors='coerce')
                        valid_areas = grain_data['area'].dropna()
                        
                        if len(valid_areas) > 0:
                            grain_data['area_um2'] = valid_areas * (scale_factor ** 2)
                            grain_data['diameter_um'] = 2 * np.sqrt(grain_data['area_um2'] / np.pi)
                
                # ä¿å­˜CSV
                csv_path = output_dir / "grain_statistics.csv"
                grain_data.to_csv(csv_path, index=False, encoding='utf-8')
                output_files.append(str(csv_path))
                self.logger.info(f"é¢—ç²’æ•°æ®ä¿å­˜è‡³: {csv_path}")
                
                # ä¿å­˜JSONæ±‡æ€»ä¿¡æ¯
                if self.config['output']['save_summary']:
                    summary = self._create_summary_dict(
                        image_path, image, grain_data, scale_detection_success, scale_factor
                    )
                    
                    json_path = output_dir / "summary.json"
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(summary, f, indent=2, ensure_ascii=False)
                    output_files.append(str(json_path))
            
            # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
            if self.config['output']['save_performance']:
                performance_data = {
                    'image_name': Path(image_path).name,
                    'processing_time': time.time() - start_time,
                    'grains_count': len(all_grains),
                    'timestamp': datetime.now().isoformat(),
                    'image_size': f"{image.shape[1]}x{image.shape[0]}"
                }
                
                perf_path = output_dir / "performance.json"
                with open(perf_path, 'w', encoding='utf-8') as f:
                    json.dump(performance_data, f, indent=2, ensure_ascii=False)
                output_files.append(str(perf_path))
            
            result['output_files'] = output_files
            result['processing_time'] = time.time() - start_time
            self.logger.info(f"å›¾ç‰‡å¤„ç†å®Œæˆï¼Œè€—æ—¶: {result['processing_time']:.2f}ç§’")
            
        except Exception as e:
            result['success'] = False
            result['error_message'] = str(e)
            result['processing_time'] = time.time() - start_time
            self.logger.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {image_path}")
            self.logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
            self.logger.error(traceback.format_exc())
            
        return result
    
    def _load_image_safely(self, image_path: str) -> Optional[np.ndarray]:
        """å®‰å…¨åŠ è½½å›¾ç‰‡ï¼ˆå¤šé‡å›é€€æœºåˆ¶ï¼‰"""
        methods = [
            self._load_with_skimage,
            self._load_with_pil,
            self._load_with_opencv,
            self._load_with_binary
        ]
        
        for method in methods:
            try:
                image = method(image_path)
                if image is not None:
                    # è½¬æ¢ä¸ºRGBæ ¼å¼
                    image = convert_to_rgb(image)
                    # å½’ä¸€åŒ–åˆ°0-255
                    image = normalize_image(image)
                    return image
            except Exception as e:
                self.logger.debug(f"å›¾ç‰‡åŠ è½½æ–¹æ³•å¤±è´¥: {method.__name__} - {e}")
                continue
        
        return None
    
    def _load_with_skimage(self, image_path: str) -> Optional[np.ndarray]:
        """ä½¿ç”¨skimageåŠ è½½"""
        try:
            from skimage import io
            image = io.imread(image_path)
            return image
        except:
            return None
    
    def _load_with_pil(self, image_path: str) -> Optional[np.ndarray]:
        """ä½¿ç”¨PILåŠ è½½"""
        try:
            pil_image = Image.open(image_path)
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            return np.array(pil_image)
        except:
            return None
    
    def _load_with_opencv(self, image_path: str) -> Optional[np.ndarray]:
        """ä½¿ç”¨OpenCVåŠ è½½"""
        try:
            import cv2
            img_bgr = cv2.imread(image_path)
            if img_bgr is not None:
                return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        except:
            return None
    
    def _load_with_binary(self, image_path: str) -> Optional[np.ndarray]:
        """ä½¿ç”¨äºŒè¿›åˆ¶åŠ è½½"""
        try:
            with open(image_path, 'rb') as f:
                data = f.read()
                pil_image = Image.open(io.BytesIO(data))
                if pil_image.mode != 'RGB':
                    pil_image = pil_image.convert('RGB')
                return np.array(pil_image)
        except:
            return None
    
    def create_output_structure(self, image_path: Path) -> Path:
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        if self.config['output']['create_subdirs']:
            image_name = image_path.stem
            output_dir = self.output_root / "images" / image_name
        else:
            output_dir = self.output_root
        
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir
    
    def _create_summary_dict(self, image_path, image, grain_data, scale_success, scale_factor):
        """åˆ›å»ºæ±‡æ€»ä¿¡æ¯å­—å…¸ï¼ˆä¸é¡¹ç›®ä¸€æ ¼å¼ç›¸åŒï¼‰"""
        summary = {
            'image_name': Path(image_path).name,
            'image_size': {
                'height': image.shape[0],
                'width': image.shape[1],
                'channels': image.shape[2]
            },
            'total_grains': int(len(grain_data)),
            'processing_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'system_version': self.VERSION
        }
        
        # é¢ç§¯ç»Ÿè®¡
        if 'area' in grain_data.columns:
            grain_data['area'] = pd.to_numeric(grain_data['area'], errors='coerce')
            valid_areas = grain_data['area'].dropna()
            
            if len(valid_areas) > 0:
                summary['area_statistics_pixels'] = {
                    'total': float(valid_areas.sum()),
                    'average': float(valid_areas.mean()),
                    'min': float(valid_areas.min()),
                    'max': float(valid_areas.max()),
                    'std': float(valid_areas.std())
                }
        
        # çœŸå®é¢ç§¯ç»Ÿè®¡
        if scale_success:
            summary['scale_detection'] = {
                'success': True,
                'scale_factor_um_per_px': float(scale_factor)
            }
            
            if 'area_um2' in grain_data.columns:
                grain_data['area_um2'] = pd.to_numeric(grain_data['area_um2'], errors='coerce')
                valid_areas_um2 = grain_data['area_um2'].dropna()
                
                if len(valid_areas_um2) > 0:
                    summary['area_statistics_um2'] = {
                        'total': float(valid_areas_um2.sum()),
                        'average': float(valid_areas_um2.mean()),
                        'min': float(valid_areas_um2.min()),
                        'max': float(valid_areas_um2.max())
                    }
        
        return summary
    
    def show_system_info(self):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print("=" * 60)
        print(f"ğŸ­ SuperFastSAMå²©çŸ³é¢—ç²’è‡ªåŠ¨åˆ†å‰²ç³»ç»Ÿ v{self.VERSION}")
        print("=" * 60)
        print(f"ç³»ç»Ÿæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"è¾“å‡ºç›®å½•: {self.output_root}")
        print(f"è®¾å¤‡æ¨¡å¼: {self.config['model_paths'].get('device', 'cpu')}")
        print(f"YOLOæ¨¡å‹: {self.config['model_paths']['yolo']}")
        print(f"FastSAMæ¨¡å‹: {self.config['model_paths']['fastsam']}")
        
        scale_config = self.config.get('scale_detection', {})
        if scale_config.get('enabled', False):
            print(f"æ¯”ä¾‹å°ºæ£€æµ‹: å·²å¯ç”¨")
            print(f"å·²çŸ¥é•¿åº¦: {scale_config.get('known_length_um', 'N/A')} Î¼m")
        else:
            print(f"æ¯”ä¾‹å°ºæ£€æµ‹: å·²ç¦ç”¨")
        
        if self.grain_label_config.get('enabled', True):
            print(f"é¢—ç²’æ ‡æ³¨: å·²å¯ç”¨")
            bg_color = self.grain_label_config.get('bg_color', '')
            if bg_color is None or bg_color == '':
                print(f"æ ‡æ³¨æ ·å¼: æ— èƒŒæ™¯, {self.grain_label_config.get('font_size', 11)}pxé»„è‰²æ–‡å­—")
            else:
                print(f"æ ‡æ³¨æ ·å¼: æœ‰èƒŒæ™¯, {self.grain_label_config.get('font_size', 9)}pxé»‘è‰²æ–‡å­—")
            print(f"æœ€å¤§æ ‡æ³¨æ•°: {self.grain_label_config.get('max_labels', 1000)}")
        else:
            print(f"é¢—ç²’æ ‡æ³¨: å·²ç¦ç”¨")
        
        print(f"æ€§èƒ½ç›‘æ§: {'å·²å¯ç”¨' if self.config.get('performance', {}).get('enable_monitoring', False) else 'å·²ç¦ç”¨'}")
        print("=" * 60)


if __name__ == "__main__":
    print("è¿™æ˜¯ä¸€ä¸ªæ¨¡å—æ–‡ä»¶ï¼Œè¯·é€šè¿‡ run_super_fastsam.py æ¥å¯åŠ¨ç³»ç»Ÿ")
    print("æˆ–è€…ç›´æ¥ä½¿ç”¨:")
    print("  from rock_super_fastsam import RockSegmentationSystemSuper")
    print("  system = RockSegmentationSystemSuper()")
    print("  system.initialize_models()")
    print("  system.process_single_image('å›¾ç‰‡è·¯å¾„')")